{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../..')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, LSTM, TimeDistributed\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # convert to datetime\n",
    "    df['MEASUREMENT_TIME'] = pd.to_datetime(df['MEASUREMENT_TIME'], errors='coerce')\n",
    "    # calculate time difference\n",
    "    df['diff'] = df['MEASUREMENT_TIME'].diff().shift(-1)\n",
    "    # drop unncessary columns\n",
    "    df = df.drop(columns=['ID_INPUT', 'PRIVATE_DATA'])\n",
    "    # rename remaining columns\n",
    "    df.columns = ['time', 'window', 'diff']\n",
    "    # Select 3-week time-frame\n",
    "#     df = df[(df['time'] >= '2022-01-03') & (df['time'] <= '2022-01-24')]\n",
    "    # Remove windows appearing only once\n",
    "    df = df.groupby('window').filter(lambda x: len(x) > 1)\n",
    "    # Remove NaNs\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Series Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize_timeseries(df, freq):\n",
    "    # regularized time-series index at specified frequency\n",
    "    out_time = pd.date_range(df['time'].values[0], df['time'].values[len(df)-1], freq=freq)\n",
    "    out_windows = []\n",
    "    \n",
    "    for i in range(len(out_time[:-1])):\n",
    "        # subquery dataframe to each time-step\n",
    "        df_small = df[(df['time'] >= out_time[i]) & (df['time'] <= out_time[i+1])]\n",
    "        \n",
    "        if len(df_small) == 0:\n",
    "            # NaN if no windows in time-step\n",
    "            out_windows.append(np.NaN)\n",
    "        elif len(df_small) == 1:\n",
    "            # append window if only one window in time-step\n",
    "            out_windows.append(df_small['window'].values[0])\n",
    "        else:     \n",
    "            # append window with most time spent if multiple windows during time-step\n",
    "            summed = df_small.groupby('window')['diff'].sum().reset_index().sort_values('diff', ascending=False)            \n",
    "            out_windows.append(summed['window'].values[0])\n",
    "    \n",
    "    # create new dataframe\n",
    "    out = pd.DataFrame(list(zip(out_time, out_windows)), columns =['time', 'window']).fillna(method=\"ffill\")\n",
    "    \n",
    "    return out.set_index('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, n_unique):\n",
    "    \"\"\"one hot encode a sequence as 2-d array\"\"\"\n",
    "    encoding = list()\n",
    "    for value in sequence:\n",
    "        vector = [0 for _ in range(n_unique)]\n",
    "        vector[value] = 1\n",
    "        encoding.append(vector)\n",
    "    return np.array(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(sequence, n_in, n_out):\n",
    "    \"\"\"transform encoded sequence to supervised learning problem\"\"\"\n",
    "    # create lag copies of the sequence\n",
    "    df = pd.DataFrame(sequence)\n",
    "    df = pd.concat([df.shift(n_in-i-1) for i in range(n_in)], axis=1)\n",
    "    # drop rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "    # specify columns for input and output pairs\n",
    "    values = df.values\n",
    "    width = sequence.shape[1]\n",
    "    X = values.reshape(len(values), n_in, width)\n",
    "    y = values[:, 0:(n_out*width)].reshape(len(values), n_out, width)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_decode(encoded_seq):\n",
    "    \"\"\"decode a one hot encoded string\"\"\"\n",
    "    return [np.argmax(vector) for vector in encoded_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(pred, test, dim, enc):\n",
    "    \"\"\"decode all one-hot encoded strings and store as dataframe\"\"\"\n",
    "    preds = [] \n",
    "    for i in range(len(test[dim-1:])):\n",
    "        preds.append(one_hot_decode(pred[i])[0])\n",
    "        \n",
    "    return pd.DataFrame(enc.inverse_transform(preds), index=test.index[dim-1:], columns=['window'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_time_series_plot(df_train, df_valid, df_test, df_pred, freq, mode='markers'):\n",
    "    \"\"\"Final App Launch+Usage Predictions / Total Time Spent\"\"\"\n",
    "    trace1 = go.Scatter(\n",
    "        x = df_train.index,\n",
    "        y = df_train.window,\n",
    "        mode = mode,\n",
    "        marker = dict(color = 'royalblue'),\n",
    "        name = 'Training Data'\n",
    "    )\n",
    "    trace2 = go.Scatter(\n",
    "        x = df_valid.index,\n",
    "        y = df_valid.window,\n",
    "        mode = mode,\n",
    "        marker = dict(color = 'mediumpurple'),\n",
    "        name = 'Validation Data'\n",
    "    )\n",
    "    trace3 = go.Scatter(\n",
    "        x = df_test.index,\n",
    "        y = df_test.window,\n",
    "        mode = mode,\n",
    "        marker = dict(color = 'red'),\n",
    "        name = 'Testing Data'\n",
    "    )\n",
    "    trace4 = go.Scatter(\n",
    "        x = df_pred.index,\n",
    "        y = df_pred.window,\n",
    "        mode = mode,\n",
    "        marker = dict(color = 'mediumseagreen'),\n",
    "        name = 'Predictions'\n",
    "    )\n",
    "    layout = go.Layout(\n",
    "        title = \"App Usage Predictions at {} Intervals\".format(freq),\n",
    "        xaxis = {'title' : \"Time\"},\n",
    "        yaxis = {'title' : \"App Executable\"}\n",
    "    )\n",
    "    \n",
    "    return go.Figure(data=[trace1, trace2, trace3, trace4], layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_models():\n",
    "    #list users\n",
    "    users = ['user1', 'user2', 'intel']\n",
    "    #regularization frequencies\n",
    "    freqs = ['10s', '30s', '1min']\n",
    "    #hyper-parameters\n",
    "    look_backs = [3, 6, 12]\n",
    "    nodes = [16, 32, 64]\n",
    "    batch_sizes = [6, 12, 24]\n",
    "    for user in users:\n",
    "        #number of models tested\n",
    "        count = 1\n",
    "        #outputs dictionary\n",
    "        outputs = {'frequency': [], 'look_back':[], 'num_nodes':[], 'batch_size':[], \n",
    "                   'accuracy':[], 'balanced_accuracy':[], 'weighted_f1_score':[], \n",
    "                   'weighted_precision':[], 'weighted_recall':[], 'time': []}\n",
    "        #load data for user\n",
    "        df = pd.read_csv('data/{}_window_data.csv'.format(user))\n",
    "        #clean user window data\n",
    "        df = clean_data(df)\n",
    "        for freq in freqs:\n",
    "            #regularize time-series by frequency\n",
    "            df_regular = regularize_timeseries(df, freq)\n",
    "            #split regular data into train and test for later time-series evaluation\n",
    "            #60-20-20 split for train-validation-test\n",
    "            df_train_regular, df_test_regular = train_test_split(df_regular, test_size=0.4, shuffle=False)\n",
    "            df_valid_regular, df_test_regular = train_test_split(df_test_regular, test_size=0.5, shuffle=False)\n",
    "            #transform categorical labels to numerical\n",
    "            enc = LabelEncoder()\n",
    "            df_labeled = enc.fit_transform(df_regular)\n",
    "            encoded_length = len(enc.classes_)\n",
    "            #one-hot encode numerical labels\n",
    "            df_encoded = one_hot_encode(df_labeled, encoded_length)\n",
    "            #split one-hot encoded arrays into train and test\n",
    "            #60-20-20 split for train-validation-test\n",
    "            df_train, df_test = train_test_split(df_encoded, test_size=0.4, shuffle=False)\n",
    "            df_valid, df_test = train_test_split(df_test, test_size=0.5, shuffle=False)\n",
    "            for look_back in look_backs:\n",
    "                #specify different look_back values for sequence input and prediction length\n",
    "                #convert to supervised learning problem\n",
    "                X_train, y_train = to_supervised(df_train, look_back, look_back)\n",
    "                X_valid, y_valid = to_supervised(df_valid, look_back, look_back)\n",
    "                X_test, y_test = to_supervised(df_test, look_back, look_back)\n",
    "                #specify different number of nodes in LSTM layer\n",
    "                for node in nodes:\n",
    "                    #specify different batch sizes for model fitting\n",
    "                    for batch_size in batch_sizes:\n",
    "                        #instantiate Sequential model\n",
    "                        model = Sequential()\n",
    "                        model.add(LSTM(node, input_shape=(look_back, encoded_length), return_sequences=True))\n",
    "                        model.add(TimeDistributed(Dense(encoded_length, activation='softmax')))\n",
    "                        model.compile(optimizer='adam', \n",
    "                                      loss='categorical_crossentropy', \n",
    "                                      metrics='categorical_accuracy')\n",
    "                        #optimal epochs selected based on epoch-loss curve\n",
    "                        start = time()\n",
    "                        history = model.fit(X_train, y_train, \n",
    "                                  epochs=10, \n",
    "                                  batch_size=batch_size, \n",
    "                                  validation_data=(X_valid, y_valid),\n",
    "                                  verbose=0, \n",
    "                                  shuffle=False)\n",
    "                        #save model training logs in dataframe\n",
    "                        end = time()\n",
    "                        log = pd.DataFrame(history.history)\n",
    "                        log.to_csv(\"outputs/LSTM/tables/{}_model_{}_logs.csv\".format(user,count))\n",
    "                        #make predictions on test data\n",
    "                        y_pred = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "                        df_pred = decode_predictions(y_pred, df_test_regular, look_back, enc)\n",
    "                        #evaluate predictions\n",
    "                        accuracy = accuracy_score(df_pred, df_test_regular[look_back-1:])\n",
    "                        balanced_accuracy = balanced_accuracy_score(df_pred, df_test_regular[look_back-1:])\n",
    "                        f1 = f1_score(df_pred, df_test_regular[look_back-1:], average='weighted')\n",
    "                        precision = precision_score(df_pred, df_test_regular[look_back-1:], average='weighted')\n",
    "                        recall = recall_score(df_pred, df_test_regular[look_back-1:], average='weighted')\n",
    "                        #save parameters and evaluations\n",
    "                        outputs['frequency'].append(freq)\n",
    "                        outputs['look_back'].append(look_back)\n",
    "                        outputs['num_nodes'].append(node)\n",
    "                        outputs['batch_size'].append(batch_size)\n",
    "                        outputs['accuracy'].append(accuracy)\n",
    "                        outputs['balanced_accuracy'].append(balanced_accuracy)\n",
    "                        outputs['weighted_f1_score'].append(f1)\n",
    "                        outputs['weighted_precision'].append(precision)\n",
    "                        outputs['weighted_recall'].append(recall)\n",
    "                        outputs['time'].append(end-start)\n",
    "                        #save figures\n",
    "                        fig = make_time_series_plot(df_train_regular, df_valid_regular, df_test_regular, \n",
    "                                                    df_pred, freq)\n",
    "                        fig.write_image(\"outputs/LSTM/plots/{}_model_{}_dot_plot.png\".format(user,count))\n",
    "                        fig = make_time_series_plot(df_train_regular, df_valid_regular, df_test_regular, \n",
    "                                                    df_pred, freq, mode='lines')\n",
    "                        fig.write_image(\"outputs/LSTM/plots/{}_model_{}_line_plot.png\".format(user,count))\n",
    "                        #increment model count\n",
    "                        print(count, end='\\r')\n",
    "                        count+=1\n",
    "    \n",
    "        pd.DataFrame(outputs).to_csv(\"outputs/LSTM/tables/{}_model_outputs.csv\".format(user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate_all_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
